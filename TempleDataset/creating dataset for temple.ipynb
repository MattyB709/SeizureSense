{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not enabled in config, skipping initialization\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import json\n",
    "import torch\n",
    "mne.set_log_level('ERROR')\n",
    "mne.cuda.init_cuda(verbose=True)\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a lisst of every csv and its corresponding edf\n",
    "csv_file_paths=[r\"C:\\Users\\uddha\\OneDrive\\Desktop\\SS Learning\\Se\\SeizureSense\\TempleDataset\\aaaaaaac_s001_t000 (1).csv\"]\n",
    "edf_file_paths=[r\"C:\\Users\\uddha\\Downloads\\chb02_19.edf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay so i needd to create a dicitionary for each channel becayse thats the way its in a row with it being a \n",
    "# different row for each channel that has a new label in it  \n",
    "# ex: \n",
    "# FP1-F7,0.0000,36.8868,bckg,1.0000\n",
    "#FP1-F7,36.8868,183.3055,cpsz,1.0000\n",
    "#FP1-F7,183.3055,301.0000,bckg,1.0000\n",
    "#all of these are one channel but its displayed as rows. \n",
    "# i also need to check if in the channel name it has a LE or an AR reference and if the channel doesnt have \n",
    "#19 non referenced channels I drop it from the list of csv paths and edf paths\n",
    "#so i have to create a dictionary for each channel and make sure it has the same label for each of the time step\n",
    "#and make sure they are all the same plus or minus 5 seceonds then create the real dictionary with the format: \n",
    "#'CHB-MIT/CHB_Database/chb01/chb01_03.edf': [(0, 2396, 'interictal'), (2396, 2996, 'preictal'), (2996, 3036, 'ictal'), (3036, 3600, 'interictal')],\n",
    "#and append that to the master dictionary that gets created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Dictionary for temple dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_edf_channels(raw, required_channels=19):\n",
    "    \n",
    "    channels_to_drop = [ch_name for ch_name in raw.ch_names if \"LE\" in ch_name or \"AR\" in ch_name]\n",
    "    num_valid_channels = len(raw.ch_names) - len(channels_to_drop)\n",
    "    if len(channels_to_drop)>0:\n",
    "        raw.drop_channels(channels_to_drop)\n",
    "    if num_valid_channels >= required_channels:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def calculate_seizure_times_by_channel(df):\n",
    "    \n",
    "    seizure_df = df[df['label'] == 'cpsz']\n",
    "    seizure_start_times = []\n",
    "    seizure_end_times = []\n",
    "    \n",
    "    # Group by channel and process each group\n",
    "    for channel, group in seizure_df.groupby('channel'):\n",
    "        # Sort the group by start time to ensure chronological order\n",
    "        group = group.sort_values('start_time')\n",
    "        \n",
    "        # Initialize temporary lists to store individual seizure times for the current channel\n",
    "        temp_start_times = []\n",
    "        temp_end_times = []\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            # Append start and end times to temporary lists\n",
    "            temp_start_times.append(row['start_time'])\n",
    "            temp_end_times.append(row['stop_time'])\n",
    "        \n",
    "        # Calculate the average start and end times for seizures in this channel\n",
    "        if temp_start_times:\n",
    "            seizure_start_times.append(np.mean(temp_start_times))\n",
    "            seizure_end_times.append(np.mean(temp_end_times))\n",
    "    \n",
    "    # Round the average times to nearest second and ensure they are distinct\n",
    "    seizure_start_times = [round(time) for time in set(seizure_start_times)]\n",
    "    seizure_end_times = [round(time) for time in set(seizure_end_times)]\n",
    "    \n",
    "    return seizure_start_times, seizure_end_times\n",
    "\n",
    "def create_labels_dict(edf_file_path, seizure_start_times, seizure_end_times, total_duration):\n",
    "    # Initialize the list to store the time periods and labels\n",
    "    labels = []\n",
    "    \n",
    "    # Start with an 'interictal' period if the first seizure starts after 0\n",
    "    if seizure_start_times and seizure_start_times[0] > 600:\n",
    "        labels.append((0, seizure_start_times[0] - 600, 'interictal'))\n",
    "    \n",
    "    # Process each seizure\n",
    "    for start, end in zip(seizure_start_times, seizure_end_times):\n",
    "        preictal_start = max(start - 600, 0)  # Ensure preictal doesn't start before 0\n",
    "        if labels and labels[-1][1] < preictal_start:\n",
    "            # Add an 'interictal' period before 'preictal' if there's a gap\n",
    "            labels.append((labels[-1][1], preictal_start, 'interictal'))\n",
    "        labels.append((preictal_start, start, 'preictal'))\n",
    "        labels.append((start, end, 'ictal'))\n",
    "    \n",
    "    # End with an 'interictal' period from the last 'ictal' or 'preictal' to the end\n",
    "    if labels:\n",
    "        final_end_time = labels[-1][1]\n",
    "        if final_end_time < total_duration:\n",
    "            labels.append((final_end_time, total_duration, 'interictal'))\n",
    "    else:\n",
    "        # If there are no seizures, mark the entire duration as 'interictal'\n",
    "        labels.append((0, total_duration, 'interictal'))\n",
    "    \n",
    "    return {edf_file_path: labels}\n",
    "\n",
    "\n",
    "\n",
    "def create_dictionaries(edf_file_path,csv_file_path):\n",
    "\n",
    "        \n",
    "    seizure_start_times, seizure_end_times=calculate_seizure_times_by_channel(df)\n",
    "    labels_dict=create_labels_dict(edf_file_path, seizure_start_times, seizure_end_times, total_duration)\n",
    "    return labels_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uddha\\AppData\\Local\\Temp\\ipykernel_15532\\2608346524.py:3: RuntimeWarning: Channel names are not unique, found duplicates for: {'T8-P8'}. Applying running numbers for duplicates.\n",
      "  raw=mne.io.read_raw_edf(edf_file_path,preload=True,verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9', 'FT9-FT10', 'FT10-T8', 'T8-P8-1']\n",
      "{'C:\\\\Users\\\\uddha\\\\Downloads\\\\chb02_19.edf': [(0, 37, 'preictal'), (37, 160, 'ictal'), (160, 3600.0, 'interictal')]}\n"
     ]
    }
   ],
   "source": [
    "master_labels_dict = {}\n",
    "for edf_file_path, csv_file_path in zip(edf_file_paths,csv_file_paths):\n",
    "    raw=mne.io.read_raw_edf(edf_file_path,preload=True,verbose=False)\n",
    "    print(raw.ch_names)\n",
    "    raw=raw.resample(200,npad='auto')\n",
    "    total_duration=raw.n_times/200\n",
    "    \n",
    "    df=pd.read_csv(csv_file_path,comment=\"#\")\n",
    "    \n",
    "    if not validate_edf_channels(raw):\n",
    "        csv_file_paths.remove(csv_file_path)\n",
    "        edf_file_paths.remove(edf_file_path)\n",
    "        continue\n",
    "    labels_dict=create_dictionaries(edf_file_path, csv_file_path)\n",
    "    master_labels_dict.update(labels_dict)\n",
    "    \n",
    "print(master_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9', 'FT9-FT10', 'FT10-T8', 'T8-P8-1']\n",
      "['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9', 'FT9-FT10', 'FT10-T8', 'T8-P8-1']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "raw=mne.io.read_raw_edf(r\"C:\\Users\\uddha\\Downloads\\chb02_19.edf\",preload=True)\n",
    "file_path=r\"C:\\Users\\uddha\\Downloads\\aaaaaaac_s001_t001.edf\"\n",
    "print(raw.ch_names)\n",
    "print(validate_edf_channels(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw):\n",
    "    raw=raw.resample(200)\n",
    "    eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True)\n",
    "\n",
    "\n",
    "    freqs = (60)\n",
    "    raw_notch = raw.copy().notch_filter(freqs=freqs, picks=eeg_picks)\n",
    "    \n",
    "    raw.notch_filter(freqs=freqs, picks=eeg_picks)  # Apply notch filter\n",
    "    raw.filter(l_freq=30, h_freq=None, fir_design='firwin', filter_length='auto', phase='zero', fir_window='hamming')\n",
    "    numpy_array=raw.get_data()\n",
    "    segment_tensor=torch.from_numpy(numpy_array).float().unsqueeze(0).unsqueeze(0)\n",
    "    return(segment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 33, 200])\n"
     ]
    }
   ],
   "source": [
    "file_path=r\"C:\\Users\\uddha\\Downloads\\aaaaaaac_s001_t001.edf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edf_file_path in edf_file_paths:\n",
    "    raw=mne.io.read_raw_edf(edf_file_path,preload=True)\n",
    "    tensor=preprocessing(raw)\n",
    "    #save one seceond segments as pt files\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
