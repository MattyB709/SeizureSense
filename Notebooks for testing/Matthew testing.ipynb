{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not enabled in config, skipping initialization\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import json\n",
    "import time\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "mne.cuda.init_cuda(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to read our json file of each dictionary mapped to it's file name\n",
    "def read_dict_from_json_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHBData(Dataset):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        \n",
    "        self.segment_dict = read_dict_from_json_file('\\CHB-MIT\\Segment_dicts\\segment_dict_laptop.json')\n",
    "        # Mapping label names to integers\n",
    "        self.label_to_int = {'interictal': 0, 'preictal': 1, 'ictal': 2}\n",
    "        self.alpha = alpha  # Smoothing factor for EMA. Smaller alpha places greater importance on more recent elements\n",
    "        # Initialize EMA and variance (for standard deviation calculation) for each channel\n",
    "        self.EMA = {}\n",
    "        self.variance = {}\n",
    "        # Initial values could be adjusted based on dataset characteristics\n",
    "        self.initialized_channels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segment_dict)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segment_filepath = f'CHB-MIT/Segments/chb01/{index}-chb01.pt'\n",
    "        segment = torch.load(segment_filepath)\n",
    "        segment = segment.to(device)\n",
    "        segment = self.pad_sequence(segment)\n",
    "        \n",
    "        # Ensure EMA and variance are initialized\n",
    "        if not self.initialized_channels:\n",
    "            self.initialize_ema_variance(segment.shape[1])\n",
    "            self.initialized_channels = True\n",
    "\n",
    "        # Computes z-normalization based on Exponential moving average\n",
    "        self.ema_normalize(segment)\n",
    "\n",
    "        # Retrieve the label and map it to an integer\n",
    "        label_name = self.segment_dict[segment_filepath]\n",
    "        label = self.label_to_int[label_name]\n",
    "        \n",
    "        segment = segment.float()  # Assuming you're using a device in the outer scope\n",
    "\n",
    "        return segment, label\n",
    "\n",
    "    def pad_sequence(self, segment):\n",
    "        total_padding = 28 - segment.shape[1]\n",
    "        pad_top = total_padding // 2\n",
    "        pad_bottom = total_padding - pad_top\n",
    "        padded_tensor = F.pad(segment, (0, 0, pad_top, pad_bottom), mode='constant', value=0)\n",
    "        return padded_tensor\n",
    "\n",
    "    def initialize_ema_variance(self, num_channels):\n",
    "        for i in range(num_channels):\n",
    "            self.EMA[i] = None\n",
    "            self.variance[i] = None\n",
    "\n",
    "    def ema_normalize(self, segment):\n",
    "        # Update EMA and standard deviation for each channel and normalize\n",
    "        for i in range(segment.shape[1]):  \n",
    "            channel_data = segment[0, i, :] # select each channel\n",
    "            if self.EMA[i] is None:  # First update\n",
    "                self.EMA[i] = channel_data.mean()\n",
    "                self.variance[i] = channel_data.var()\n",
    "            else:\n",
    "                #update EMA and variance based\n",
    "                self.EMA[i] = self.alpha * channel_data.mean() + (1 - self.alpha) * self.EMA[i]\n",
    "                self.variance[i] = self.alpha * ((channel_data - self.EMA[i])**2).mean() + (1 - self.alpha) * self.variance[i]\n",
    "\n",
    "            std = torch.sqrt(self.variance[i])\n",
    "            # Normalize this channel\n",
    "            segment[0,i, :] = (channel_data - self.EMA[i]) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\CHB-MIT\\\\Segment_dicts\\\\segment_dict_laptop.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCHBData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the dataset into train and test sets\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_indices, test_indices \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(full_dataset)), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m, in \u001b[0;36mCHBData.__init__\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_dict \u001b[38;5;241m=\u001b[39m \u001b[43mread_dict_from_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCHB-MIT\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSegment_dicts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msegment_dict_laptop.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Mapping label names to integers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_to_int \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterictal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreictal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mictal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mread_dict_from_json_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_dict_from_json_file\u001b[39m(filepath):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/anaconda3/envs/mne/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\CHB-MIT\\\\Segment_dicts\\\\segment_dict_laptop.json'"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "full_dataset = CHBData()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_indices, test_indices = train_test_split(range(len(full_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "#  split the test set into validation and actual test sets\n",
    "val_indices, test_indices = train_test_split(test_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# dataloaders for training, validation, and test\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Instantiate dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=num_workers, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_workers, drop_last=True)\n",
    "#training_dataset = CHBData(segment_dict)\n",
    "#mean, std = calculate_mean_std(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "full_dataset = CHBData(segment_dict)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_indices, val_indices = train_test_split(range(len(full_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "#  split the test set into validation and actual test sets\n",
    "train_indices, val_indices = train_test_split(range(len(full_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# dataloaders for training, validation, and test\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "# Instantiate dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=num_workers, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=num_workers, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeizureSense(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SeizureSense,self).__init__()\n",
    "        \n",
    "        #Block 1\n",
    "        #should be taking in an input of 23x512\n",
    "        #first layer temporal filters\n",
    "        self.conv1=nn.Conv2d(1,8,(1,128),stride=1,padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8, False)\n",
    "        \n",
    "        #spatial layer(depthwise layer)\n",
    "        self.conv2_23=nn.Conv2d(8,32,(28,1))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, False)\n",
    "        self.avgpool1 = nn.AvgPool2d((1,2))\n",
    "        #apply dropout here in forward\n",
    "        \n",
    "        \n",
    "        #Block 2\n",
    "        #sepereable convolutional 2d\n",
    "        self.conv3=nn.Conv2d(32,32,(1,16),stride=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32,False) #CHECK THIS!\n",
    "        self.avgpool2 = nn.AvgPool2d((1,16))\n",
    "        \n",
    "     \n",
    "        #Block 3\n",
    "        \n",
    "        self.fc1= nn.Linear(96, 30)\n",
    "        self.fc2=nn.Linear(30,3)\n",
    "\n",
    "        #apply dropout here in forward\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)    \n",
    "        x = self.conv2_23(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool1(x)\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sigmoid(self.fc2(x))  # Use torch.sigmoid instead of F.sigmoid (deprecated)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeizureSense()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      3\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "num_classes = 3\n",
    "num_patients = 16\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for patient_number in range(1,num_patients):\n",
    "        #ensures correct formatting for dataset\n",
    "        if patient_number < 10:\n",
    "            full_dataset = CHBData(f\"0{patient_number}\")\n",
    "        else:\n",
    "            full_dataset = CHBData(patient_number)\n",
    "\n",
    "        # Split the dataset into train and test sets\n",
    "        total_samples = len(full_dataset)\n",
    "        split_point = int(total_samples * 0.7)  # For a 70-30 split\n",
    "\n",
    "        train_indices = list(range(0, split_point))\n",
    "        val_indices = list(range(split_point, total_samples))\n",
    "\n",
    "        # dataloaders for training and validation\n",
    "        train_dataset = Subset(full_dataset, train_indices)\n",
    "        val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "        # Instantiate dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            aggregated_outputs = outputs.mean(dim=1)\n",
    "            labels = labels.float()\n",
    "            loss = criterion(aggregated_outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_probabilities = []  # Store probabilities for AUROC calculation\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                labels = labels.to(device)\n",
    "                logits = model(inputs)\n",
    "                aggregated_logits = logits.mean(dim=1)\n",
    "                outputs = F.softmax(aggregated_logits, dim=1)\n",
    "\n",
    "                positive_class_probabilities = outputs[:, 1]\n",
    "                predictions = torch.max(outputs, 1)[1].cpu().numpy()  # Fixed to use argmax for multi-class\n",
    "\n",
    "                val_labels.extend(labels.cpu().numpy())  # Convert to numpy array\n",
    "                val_predictions.extend(predictions)\n",
    "                val_probabilities.extend(outputs.cpu().numpy())  # Convert to numpy array\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        recall = recall_score(val_labels, val_predictions, average=None, zero_division=0)  # Adjusted for multi-class\n",
    "        # Calculate AUROC\n",
    "        # For multi-class AUROC, adjust according to your class strategy\n",
    "        val_labels_binarized = label_binarize(val_labels, classes=np.unique(val_labels))\n",
    "        auroc = roc_auc_score(val_labels_binarized, np.array(val_probabilities), multi_class='ovr')  # Adjusted for multi-class\n",
    "\n",
    "        # Timing end and calculate duration\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {np.mean(losses):.4f}, Accuracy: {accuracy:.4f}, Recall: {recall.mean():.4f}, AUROC: {auroc:.4f}, Time: {epoch_duration:.2f}s')\n",
    "\n",
    "        # Reset losses for next epoch\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027023940058479533\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "recall = recall_score(val_labels,val_predictions, average=None)[1]\n",
    "print(accuracy)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_labels = []\n",
    "val_probabilities = []  # Store probabilities for AUROC calculation\n",
    "val_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "        logits = model(inputs)\n",
    "        outputs = F.softmax(logits, dim=1)            \n",
    "        \n",
    "        positive_class_probabilities = outputs[:, 1]\n",
    "        predictions = (positive_class_probabilities.cpu().numpy() > optimal_threshold).astype(int)\n",
    "        \n",
    "        val_labels.extend(labels.cpu().numpy())  # Convert to numpy array\n",
    "        val_predictions.extend(predictions)\n",
    "        val_probabilities.extend(outputs.cpu().numpy())  # Convert to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906889619883041\n",
      "0.9812925170068028\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "recall = recall_score(val_labels,val_predictions, average=None)[1]\n",
    "\n",
    "print(accuracy)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21888\n",
      "21888\n"
     ]
    }
   ],
   "source": [
    "print(len(val_predictions))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"predictions\":val_predictions, \"labels\":val_labels})\n",
    "df.to_csv(\"sdfsdoodfio.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
