{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch import tensor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import argparse\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import scipy.io\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "'''from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torch.autograd as autograd\n",
    "from torchvision.models import vgg19'''\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# from common_spatial_pattern import csp\n",
    "import torch\n",
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (22, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.shallownet(x)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 40])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "\n",
    "d_model=40\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=d_model, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold=.34 #actual=.087\n",
    "def decode_predictions(predictions):\n",
    "    agregated_outputs=predictions.mean(dim=1)\n",
    "    probabilities=F.softmax(agregated_outputs,dim=-1)\n",
    "    #print(probabilities)\n",
    "    \n",
    "    if probabilities[0][1]>optimal_threshold:\n",
    "        return 'preictal'\n",
    "    else:\n",
    "        max_values, max_indices = torch.max(probabilities, dim=1)\n",
    "        if max_indices==0:\n",
    "            return 'interictal'\n",
    "        else:\n",
    "            return 'ictal'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_edf(\"../CHB-MIT/chb02_19.edf\", preload=True)\n",
    "\n",
    "def preprocessing(raw):\n",
    "\n",
    "    eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True)\n",
    "\n",
    "\n",
    "    freqs = (60,120)\n",
    "    raw_notch = raw.copy().notch_filter(freqs=freqs, picks=eeg_picks)\n",
    "    \n",
    "    raw.notch_filter(freqs=freqs, picks=eeg_picks)  # Apply notch filter\n",
    "    raw.filter(l_freq=30, h_freq=None, fir_design='firwin', filter_length='auto', phase='zero', fir_window='hamming')\n",
    "    numpy_array=raw.get_data()\n",
    "    segment_tensor=torch.from_numpy(numpy_array).float().unsqueeze(0).unsqueeze(0)\n",
    "    return(segment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHB_files={'../CHB-MIT/chb02_19.edf': [(0, 2769, 'interictal'), (2769, 3369, 'preictal'), (3369, 3378, 'ictal'), (3378, 3600, 'interictal')],}\n",
    "true_labels=[]\n",
    "start,end=2778,3388\n",
    "for i in range(start,end):\n",
    "    for interval in CHB_files.values():\n",
    "        for start_interval,end_interval,label in interval:\n",
    "            if start_interval <= i < end_interval:\n",
    "                true_labels.append(label)\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmodel=PatchEmbedding().to(device)\n",
    "model=Mamba(d_model=40).to(device)\n",
    "\n",
    "segment_tensor=preprocessing(raw)\n",
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "\n",
    "    for i in range(2778,3388):\n",
    "        input=segment_tensor[:,:,:,256*i:256*(i+1)].float().to(device)\n",
    "        cnn_output=CNNmodel(input)\n",
    "        print(cnn_output.shape)\n",
    "        output=model(cnn_output)\n",
    "        output.mean(axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        predicted_label_names=decode_predictions(output)\n",
    "        predictions.append(predicted_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal', 'ictal']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input= torch.rand(1,1,28,256).float().\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
