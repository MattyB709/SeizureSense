{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeizureSense, our primary notebook including our dataset, model, and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not enabled in config, skipping initialization\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import json\n",
    "import time\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "mne.cuda.init_cuda(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to read our json file of each dictionary mapped to its file name\n",
    "def read_dict_from_json_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHBData(Dataset):\n",
    "    def __init__(self, patient_number, alpha=0.1, ):\n",
    "        self.patient_number = patient_number\n",
    "        self.segment_dict = read_dict_from_json_file(f'CHB-MIT/Segment_dicts/segment_dict{self.patient_number}.json')\n",
    "        # Mapping label names to integers\n",
    "        self.label_to_int = {'interictal': 0, 'preictal': 1, 'ictal': 2}\n",
    "        self.alpha = alpha  # Smoothing factor for EMA. Smaller alpha places greater importance on more recent elements\n",
    "        # Initialize EMA and variance (for standard deviation calculation) for each channel\n",
    "        self.EMA = {}\n",
    "        self.variance = {}\n",
    "        # Initial values could be adjusted based on dataset characteristics\n",
    "        self.initialized_channels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segment_dict)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segment_filepath = f'CHB-MIT/Segments/chb{self.patient_number}/{index}-chb{self.patient_number}.pt'\n",
    "        segment = torch.load(segment_filepath)\n",
    "        segment = segment.to(device)\n",
    "        segment = self.pad_sequence(segment)\n",
    "        \n",
    "        # Ensure EMA and variance are initialized\n",
    "        if not self.initialized_channels:\n",
    "            self.initialize_ema_variance(segment.shape[1])\n",
    "            self.initialized_channels = True\n",
    "\n",
    "        # Computes z-normalization based on Exponential moving average\n",
    "        self.ema_normalize(segment)\n",
    "\n",
    "        # Retrieve the label and map it to an integer\n",
    "        label_name = self.segment_dict[segment_filepath]\n",
    "        label = self.label_to_int[label_name]\n",
    "        \n",
    "        segment = segment.float()  # Assuming you're using a device in the outer scope\n",
    "\n",
    "        return segment, label\n",
    "\n",
    "    def pad_sequence(self, segment):\n",
    "        total_padding = 28 - segment.shape[1]\n",
    "        pad_top = total_padding // 2\n",
    "        pad_bottom = total_padding - pad_top\n",
    "        padded_tensor = F.pad(segment, (0, 0, pad_top, pad_bottom), mode='constant', value=0)\n",
    "        return padded_tensor\n",
    "\n",
    "    def initialize_ema_variance(self, num_channels):\n",
    "        for i in range(num_channels):\n",
    "            self.EMA[i] = None\n",
    "            self.variance[i] = None\n",
    "\n",
    "    def ema_normalize(self, segment):\n",
    "        # Update EMA and standard deviation for each channel and normalize\n",
    "        for i in range(segment.shape[1]):  \n",
    "            channel_data = segment[0, i, :] # select each channel\n",
    "            if self.EMA[i] is None:  # First update\n",
    "                self.EMA[i] = channel_data.mean()\n",
    "                self.variance[i] = channel_data.var()\n",
    "            else:\n",
    "                #update EMA and variance based\n",
    "                self.EMA[i] = self.alpha * channel_data.mean() + (1 - self.alpha) * self.EMA[i]\n",
    "                self.variance[i] = self.alpha * ((channel_data - self.EMA[i])**2).mean() + (1 - self.alpha) * self.variance[i]\n",
    "\n",
    "            std = torch.sqrt(self.variance[i])\n",
    "            # Normalize this channel\n",
    "            segment[0,i, :] = (channel_data - self.EMA[i]) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeizureSense(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SeizureSense,self).__init__()\n",
    "        \n",
    "        #Block 1\n",
    "        #should be taking in an input of 23x512\n",
    "        #first layer temporal filters\n",
    "        self.conv1=nn.Conv2d(1,8,(1,128),stride=1,padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8, False)\n",
    "        \n",
    "        #spatial layer(depthwise layer)\n",
    "        self.conv2_23=nn.Conv2d(8,32,(28,1))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, False)\n",
    "        self.avgpool1 = nn.AvgPool2d((1,2))\n",
    "        #apply dropout here in forward\n",
    "        \n",
    "        \n",
    "        #Block 2\n",
    "        #sepereable convolutional 2d\n",
    "        self.conv3=nn.Conv2d(32,32,(1,16),stride=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32,False) #CHECK THIS!\n",
    "        self.avgpool2 = nn.AvgPool2d((1,16))\n",
    "        \n",
    "     \n",
    "        #Block 3\n",
    "        \n",
    "        self.fc1= nn.Linear(96, 30)\n",
    "        self.fc2=nn.Linear(30,3)\n",
    "\n",
    "        #apply dropout here in forward\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)    \n",
    "        x = self.conv2_23(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool1(x)\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sigmoid(self.fc2(x))  # Use torch.sigmoid instead of F.sigmoid (deprecated)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeizureSense().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[219], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     53\u001b[0m aggregated_logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggregated_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m positive_class_probabilities \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     57\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Fixed to use argmax for multi-class\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\mne\\Lib\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "num_classes = 3\n",
    "num_patients = 16\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for patient_number in range(1,num_patients):\n",
    "        #ensures correct formatting for dataset\n",
    "        if patient_number < 10:\n",
    "            full_dataset = CHBData(f\"0{patient_number}\")\n",
    "        else:\n",
    "            full_dataset = CHBData(patient_number)\n",
    "\n",
    "        # Split the dataset into train and test sets\n",
    "        total_samples = len(full_dataset)\n",
    "        split_point = int(total_samples * 0.7)  # For a 70-30 split\n",
    "\n",
    "        train_indices = list(range(0, split_point))\n",
    "        val_indices = list(range(split_point, total_samples))\n",
    "\n",
    "        # dataloaders for training and validation\n",
    "        train_dataset = Subset(full_dataset, train_indices)\n",
    "        val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "        # Instantiate dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            aggregated_outputs = outputs.mean(dim=1)\n",
    "            labels = labels.float()\n",
    "            loss = criterion(aggregated_outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_probabilities = []  # Store probabilities for AUROC calculation\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                labels = labels.to(device)\n",
    "                logits = model(inputs)\n",
    "                aggregated_logits = logits.mean(dim=1)\n",
    "                outputs = F.softmax(aggregated_logits, dim=1)\n",
    "\n",
    "                positive_class_probabilities = outputs[:, 1]\n",
    "                predictions = torch.max(outputs, 1)[1].cpu().numpy()  # Fixed to use argmax for multi-class\n",
    "\n",
    "                val_labels.extend(labels.cpu().numpy())  # Convert to numpy array\n",
    "                val_predictions.extend(predictions)\n",
    "                val_probabilities.extend(outputs.cpu().numpy())  # Convert to numpy array\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        recall = recall_score(val_labels, val_predictions, average=None, zero_division=0)  # Adjusted for multi-class\n",
    "        # Calculate AUROC\n",
    "        # For multi-class AUROC, adjust according to your class strategy\n",
    "        val_labels_binarized = label_binarize(val_labels, classes=np.unique(val_labels))\n",
    "        auroc = roc_auc_score(val_labels_binarized, np.array(val_probabilities), multi_class='ovr')  # Adjusted for multi-class\n",
    "\n",
    "        # Timing end and calculate duration\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {np.mean(losses):.4f}, Accuracy: {accuracy:.4f}, Recall: {recall.mean():.4f}, AUROC: {auroc:.4f}, Time: {epoch_duration:.2f}s')\n",
    "\n",
    "        # Reset losses for next epoch\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'SeizureSenseStateDict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_threshold=.3 #actual=.087\n",
    "def decode_predictions(predictions):\n",
    "    agregated_outputs=predictions.mean(dim=1)\n",
    "    probabilities=F.softmax(agregated_outputs,dim=-1)\n",
    "    print(probabilities)\n",
    "    if probabilities[0][1]>optimal_threshold:\n",
    "        return 'preictal'\n",
    "    else:\n",
    "        max_values, max_indices = torch.max(probabilities, dim=1)\n",
    "        if max_indices==0:\n",
    "            return 'interictal'\n",
    "        else:\n",
    "            return 'ictal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 3])\n",
      "tensor([[1., 0., 0.]], device='cuda:0')\n",
      "interictal\n"
     ]
    }
   ],
   "source": [
    "input=torch.rand((1,1,28,256)).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    predictions = model(input.to(device))\n",
    "    print(predictions.shape)\n",
    "    #print(predictions)\n",
    "    \n",
    "    print(decode_predictions(predictions))\n",
    "    #predictions=decode_predictions(predictions)\n",
    "    #print('Predictions:',predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
