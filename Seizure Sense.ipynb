{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset In Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataset\n",
    "class CHBData(Dataset):\n",
    "    def __init__(self, CHB_files, segment_length):\n",
    "        self.segments = [] # input for model containing segment of length \"l\" and torch tensor containing the eeg values for that segment\n",
    "        self.labels = [] # output of model contrainig  the labels of each segment, either interictal, preictal, or ictal\n",
    "        for file_path, info in CHB_files.items():\n",
    "            processed_data=self.preprocessing(file_path) # preprocess every file in dictionary \n",
    "            for start, end, label in info:\n",
    "                segmented_eeg = self.segment_eeg(processed_data,start, end, label,segment_length) # segments that file\n",
    "                for segment,label in segmented_eeg:\n",
    "                    self.segments.append(segment) # adds to final list for model \n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.segments[i],self.labels[i]\n",
    "\n",
    "    def segment_eeg(self,segment_tensor,start, end, label,segment_length):\n",
    "        start=start*256 #have to mulyiply time by sampling rate BUDDDDYYYY \n",
    "        end=end*256\n",
    "        segments=[] # list to store tuple of each segments pytorch tensor and each label inside the data \n",
    "        for i in range(start,end,segment_length):\n",
    "            segment_end=min(i+segment_length,end)\n",
    "            segment=segment_tensor[:,i:segment_end] # isolates segment of wtv lenght it is from each torch tensor in preprocessing our data \n",
    "            segments.append((segment,label))\n",
    "            \n",
    "        return segments\n",
    "\n",
    "    \n",
    "    def preprocessing(self,file_path):\n",
    "        # loading data: \n",
    "        raw = mne.io.read_raw_edf(file_path)\n",
    "        raw.load_data()\n",
    "        # processing every raw object to remove 60 hz and its multiples:\n",
    "        eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True)\n",
    "        freqs = (60,120)\n",
    "        raw_notch = raw.copy().notch_filter(freqs=freqs, picks=eeg_picks)\n",
    "        # applying a high pass filter of order 4 with a cutoff frequency of 30 Hz to the data to enhance gamma signal to noise ratio:\n",
    "        raw_notch.filter(l_freq=30, h_freq=None, fir_design='firwin', filter_length='auto', phase='zero', fir_window='hamming')\n",
    "        numpy_array=raw_notch.get_data()\n",
    "        segment_tensor=torch.from_numpy(numpy_array)\n",
    "        return segment_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary for File Path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "CHB_files = {\n",
    "    'CHB-MIT/chb01_01.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_02.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_03.edf': [(0, 2396, 'interictal'), (2396, 2996, 'preictal'), (2996, 3036, 'ictal'), (3036, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_04.edf': [(0, 867, 'interictal'), (867, 1467, 'preictal'), (1467, 1494, 'ictal'), (1494, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_05.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_06.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_07.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_08.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_09.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_10.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_11.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_12.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_13.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_14.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_15.edf': [(0, 1132, 'interictal'), (1132, 1732, 'preictal'), (1732, 1772, 'ictal'), (1772, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_16.edf': [(0, 415, 'interictal'), (415, 1015, 'preictal'), (1015, 1066, 'ictal'), (1066, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_17.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_18.edf': [(0, 1120, 'interictal'), (1120, 1720, 'preictal'), (1720, 1810, 'ictal'), (1810, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_19.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_20.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_21.edf': [(0, 327, 'preictal'), (327, 420, 'ictal'), (420, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_22.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_23.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_24.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_25.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_26.edf': [(0, 1262, 'interictal'), (1262, 1862, 'preictal'), (1862, 1963, 'ictal'), (1963, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_27.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_29.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_30.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_31.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_32.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_33.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_34.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_36.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_37.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_38.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_39.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_40.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_41.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_42.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_43.edf': [(0, 3600, 'interictal')],\n",
    "    'CHB-MIT/chb01_46.edf': [(0, 3600, 'interictal')]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader for CHB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Nut big brain shit right here\n",
    "len(CHBData(CHB_files, 512))\n",
    "'''Okay so when we do the input yk how we changed it 512 because segment lenght is 2 and our sampling rate is \n",
    "256 so we have to multiply by sampling yp get data points right\n",
    "we have to do that for start and end time bruhh\n",
    "like we need 3600*256/512=segments per file yfm? so we need to multiply start and end time by 256 in segment function\n",
    "which is what i ended up doing and it fixed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = CHBData(CHB_files, segment_length=512)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_indices, test_indices = train_test_split(range(len(full_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "#  split the test set into validation and actual test sets\n",
    "val_indices, test_indices = train_test_split(test_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# dataloaders for training, validation, and test\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Create dataloader type shit\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeizureSense(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SeizureSense, self).__init__()\n",
    "        \n",
    "        #Block 1\n",
    "        #should be taking in an input of 23x512\n",
    "        #first layer temporal shittt(pointwise is the technical term)\n",
    "        self.conv1=nn.Conv2d(1,8,(1,128),stride=2,padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8, False)\n",
    "        \n",
    "        #spatial layer(depthwise layer)\n",
    "        self.conv2=nn.Conv2d(8,32,(23,1))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, False)\n",
    "        self.avgpool1 = nn.AvgPool2d((1,2))\n",
    "        #apply dropout here in forward\n",
    "        \n",
    "        \n",
    "        #Block 2\n",
    "        #sepereable convolutional 2d\n",
    "        self.conv3=nn.Conv2d(8,32,(1,16))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32,False) #CHECK THIS!\n",
    "        self.avgpool2 = nn.AvgPool2d((1,16))\n",
    "        #apply dropout here in forward\n",
    "        \n",
    "        \n",
    "        \n",
    "        #bilstm layers\n",
    "        \n",
    "        #Block 3\n",
    "        #fc1 layer assumiing no bilstm layer rn can change later \n",
    "        #matthew check my math here please its 2am \n",
    "        \n",
    "        self.fc1= nn.Linear(32*1*5, 30)\n",
    "        self.fc2=nn.Linear(30,3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.elu(self.conv1(x))\n",
    "        x=self.batchnorm1(x)\n",
    "        \n",
    "        x=F.elu(self.conv2(x))\n",
    "        x=self.batchnorm2(x)\n",
    "        x=self.avgpool1(x)\n",
    "        \n",
    "        x=F.elu(self.conv3(x))\n",
    "        x=self.batchnorm3(x)\n",
    "        x=self.avgpool2(x)\n",
    "        \n",
    "        #bilstm stufff\n",
    "        \n",
    "        #Fully connected time\n",
    "        x=x.view(-1,32*1*5)\n",
    "        x=F.elu(self.fc1(x))\n",
    "        x=F.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SeizureSense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
