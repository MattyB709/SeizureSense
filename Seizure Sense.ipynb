{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not enabled in config, skipping initialization\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import time\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "mne.cuda.init_cuda(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to read our json file of each dictionary mapped to its file name\n",
    "def read_dict_from_json_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHBData(Dataset):\n",
    "    def __init__(self, patient_number, alpha=0.1, ):\n",
    "        self.patient_number = patient_number\n",
    "        self.segment_dict = read_dict_from_json_file(f'CHB-MIT/Segment_dicts/segment_dict{self.patient_number}.json')\n",
    "        # Mapping label names to integers\n",
    "        self.label_to_int = {'interictal': 0, 'preictal': 1, 'ictal': 2}\n",
    "        self.alpha = alpha  # Smoothing factor for EMA. Smaller alpha places greater importance on more recent elements\n",
    "        # Initialize EMA and variance (for standard deviation calculation) for each channel\n",
    "        #self.EMA = {}\n",
    "        #self.variance = {}\n",
    "        # Initial values could be adjusted based on dataset characteristics\n",
    "        self.initialized_channels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segment_dict)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segment_filepath = f'CHB-MIT/Segments/chb{self.patient_number}/{index}-chb{self.patient_number}.pt'\n",
    "        segment = torch.load(segment_filepath)\n",
    "        segment = segment.to(device)\n",
    "        segment = self.pad_sequence(segment)\n",
    "\n",
    "                # Ensure EMA and variance are initialized\n",
    "        #if not self.initialized_channels:\n",
    "        #    self.initialize_ema_variance(segment.shape[1])\n",
    "        #    self.initialized_channels = True\n",
    "\n",
    "        # Computes z-normalization based on Exponential moving average\n",
    "        #self.ema_normalize(segment)\n",
    "\n",
    "        # Retrieve the label and map it to an integer\n",
    "        label_name = self.segment_dict[segment_filepath]\n",
    "        label = self.label_to_int[label_name]\n",
    "        \n",
    "        segment = segment.float()  # Assuming you're using a device in the outer scope\n",
    "        \n",
    "        return segment, label\n",
    "\n",
    "    def pad_sequence(self, segment):\n",
    "        total_padding = 28 - segment.shape[1]\n",
    "        pad_top = total_padding // 2\n",
    "        pad_bottom = total_padding - pad_top\n",
    "        padded_tensor = F.pad(segment, (0, 0, pad_top, pad_bottom), mode='constant', value=0)\n",
    "        return padded_tensor\n",
    "\n",
    "    def initialize_ema_variance(self, num_channels):\n",
    "        for i in range(num_channels):\n",
    "            self.EMA[i] = None\n",
    "            self.variance[i] = None\n",
    "\n",
    "    def ema_normalize(self, segment):\n",
    "        # Update EMA and standard deviation for each channel and normalize\n",
    "        for i in range(segment.shape[1]):  \n",
    "            channel_data = segment[0, i, :] # select each channel\n",
    "            if self.EMA[i] is None:  # First update\n",
    "                self.EMA[i] = channel_data.mean()\n",
    "                self.variance[i] = channel_data.var()\n",
    "            else:\n",
    "                #update EMA and variance based\n",
    "                self.EMA[i] = self.alpha * channel_data.mean() + (1 - self.alpha) * self.EMA[i]\n",
    "                self.variance[i] = self.alpha * ((channel_data - self.EMA[i])**2).mean() + (1 - self.alpha) * self.variance[i]\n",
    "\n",
    "            std = torch.sqrt(self.variance[i])\n",
    "            # Normalize this channel\n",
    "            segment[0,i, :] = (channel_data - self.EMA[i]) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeizureSense(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SeizureSense,self).__init__()\n",
    "        \n",
    "        #Block 1\n",
    "        #should be taking in an input of 28x256\n",
    "        #first layer temporal filters\n",
    "        self.conv1=nn.Conv2d(1,8,(1,128),stride=1,padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8, False)\n",
    "        \n",
    "        #spatial layer(depthwise layer)\n",
    "        self.conv2_23=nn.Conv2d(8,32,(28,1))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, False)\n",
    "        self.avgpool1 = nn.AvgPool2d((1,2))\n",
    "        #apply dropout here in forward\n",
    "        \n",
    "        \n",
    "        #Block 2\n",
    "        #sepereable convolutional 2d\n",
    "        self.conv3=nn.Conv2d(32,32,(1,16),stride=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32,False) #CHECK THIS!\n",
    "        self.avgpool2 = nn.AvgPool2d((1,16))\n",
    "        \n",
    "     \n",
    "        #Block 3\n",
    "        \n",
    "        self.fc1= nn.Linear(96, 30)\n",
    "        self.fc2=nn.Linear(30,3)\n",
    "\n",
    "        #apply dropout here in forward\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)    \n",
    "        x = self.conv2_23(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool1(x)\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeizureSense().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     43\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Validation Phase\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mne/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mne/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([1,5,5], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "num_classes = 3\n",
    "num_patients = 16\n",
    "losses = []\n",
    "\n",
    "\n",
    "for patient_number in range(1,5):\n",
    "    #ensures correct formatting for dataset\n",
    "    if patient_number < 10:\n",
    "        full_dataset = CHBData(f\"0{patient_number}\")\n",
    "    else:\n",
    "        full_dataset = CHBData(patient_number)\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_indices, val_indices = train_test_split(range(len(full_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    # dataloaders for training, validation, and test\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # Instantiate dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "    # Instantiate dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "    start_time = time.time()  # Start timing\n",
    "    for epoch in range(num_epochs):        \n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_probabilities = []  # Store probabilities for AUROC calculation\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                labels = labels.to(device)\n",
    "                logits = model(inputs)\n",
    "                outputs = F.softmax(logits, dim=1)\n",
    "\n",
    "                # finds the class with the highest probability and returns it as a one hot encoded vector (i.e [1,0,0])\n",
    "                _, predictions = torch.max(outputs, 1) \n",
    "\n",
    "                predictions = predictions.cpu().numpy()\n",
    "                val_labels.extend(labels.cpu().numpy())  # Convert to numpy array\n",
    "                val_predictions.extend(predictions)\n",
    "                #val_probabilities.extend(outputs.cpu().numpy())  # Convert to numpy array\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        recall = recall_score(val_labels, val_predictions, average=None, zero_division=0)  \n",
    "        # Calculate AUROC\n",
    "        # For multi-class AUROC, adjust according to your class strategy\n",
    "        #val_labels_binarized = label_binarize(val_labels, classes=np.unique(val_labels))\n",
    "        #auroc = roc_auc_score(val_labels_binarized, np.array(val_probabilities), average='weighted',multi_class='ovr')  \n",
    "        # Timing end and calculate duration\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "        print(recall)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {np.mean(losses):.4f}, Accuracy: {accuracy:.4f}, Recall: {recall[1]:.4f}, Time: {epoch_duration:.2f}s')\n",
    "        #AUROC: {auroc:.4f},\n",
    "        # Reset losses for next epoch\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'SeizureSenseStateDict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_threshold=.3 #actual=.087\n",
    "def decode_predictions(predictions):\n",
    "    agregated_outputs=predictions.mean(dim=1)\n",
    "    probabilities=F.softmax(agregated_outputs,dim=-1)\n",
    "    print(probabilities)\n",
    "    if probabilities[0][1]>optimal_threshold:\n",
    "        return 'preictal'\n",
    "    else:\n",
    "        max_values, max_indices = torch.max(probabilities, dim=1)\n",
    "        if max_indices==0:\n",
    "            return 'interictal'\n",
    "        else:\n",
    "            return 'ictal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 3])\n",
      "tensor([[1., 0., 0.]], device='cuda:0')\n",
      "interictal\n"
     ]
    }
   ],
   "source": [
    "input=torch.rand((1,1,28,256)).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    predictions = model(input.to(device))\n",
    "    print(predictions.shape)\n",
    "    #print(predictions)\n",
    "    \n",
    "    print(decode_predictions(predictions))\n",
    "    #predictions=decode_predictions(predictions)\n",
    "    #print('Predictions:',predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
