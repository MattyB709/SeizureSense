{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as f\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataset\n",
    "class CHBData(Dataset):\n",
    "    def __init__(self, CHB_files, segment_length):\n",
    "        self.segments = [] #input for model containing segment of length \"l\" and torch tensor containing the eeg values for that segment\n",
    "        self.labels = []#output of model contrainig  the labels of each segment and its type of \"ictal\"\n",
    "        for file_path, info in CHB_files.items():\n",
    "            processed_data=self.preprocessing(file_path) #preprocess every file in dictionary \n",
    "            for start, end, label in info:\n",
    "                segmented_eeg = self.segment_eeg(processed_data,start, end, label,segment_length) #segments that file\n",
    "                for segment,label in segmented_eeg:\n",
    "                    self.segments.append(segment) #adds to final list for model \n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.segments[i],self.labels[i]\n",
    "\n",
    "    def segment_eeg(self,segment_tensor,start, end, label,segment_length):\n",
    "        segments=[] #list to store tuple of each segments pytorch tensor and each label inside the data \n",
    "        for i in range(start,end,segment_length):\n",
    "            segment_end=min(i+segment_length,end)\n",
    "            segment=segment_tensor[:,i:segment_end] #isolates segment of wtv lenght it is from each torch tensor in preprocessing our data \n",
    "            segments.append((segment,label))\n",
    "            \n",
    "        return segments\n",
    "\n",
    "    \n",
    "    def preprocessing(self,file_path):\n",
    "        #loading data: \n",
    "        raw = mne.io.read_raw_edf(file_path)\n",
    "        raw.load_data()\n",
    "        #proccesing every raw object to remove 60 hz and its multiples:\n",
    "        eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True)\n",
    "        freqs = (60,120)\n",
    "        raw_notch = raw.copy().notch_filter(freqs=freqs, picks=eeg_picks)\n",
    "        #applying a high pass filter of order 4 with a cutoff frequency of 30 Hz to the data to enhance gamma signal to noise ratio:\n",
    "        raw_notch.filter(l_freq=30, h_freq=None, fir_design='firwin', filter_length='auto', phase='zero', fir_window='hamming')\n",
    "        numpy_array=raw_notch.get_data()\n",
    "        segment_tensor=torch.from_numpy(numpy_array)\n",
    "        return segment_tensor\n",
    "\n",
    "#dictionary containg file path and a tuple of what time frame what thing is \"ictal\" and its label    \n",
    "CHB_files = {'CHB-MIT/chb01_01.edf':[(0, 3600, \"interictal\")], 'CHB-MIT/chb01_02.edf':[(0, 3600, \"interictal\")],}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = CHBData(CHB_files, segment_length=512)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_indices, test_indices = train_test_split(range(len(full_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "#  split the test set into validation and actual test sets\n",
    "val_indices, test_indices = train_test_split(test_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# dataloaders for training, validation, and test\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Create dataloader type shit\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CHBData' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfull_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CHBData' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "full_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Model Architecture\n",
    "\n",
    "class Net(nn.module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #first layer temporal shittt\n",
    "        self.conv1=nn.Conv2d(1,1*512*4,(4,128))\n",
    "        \n",
    "        #spatial layer\n",
    "        self.conv2=nn.Conv2d(512*4,(8*4,1,512),1))\n",
    "        \n",
    "        def forward(self,x):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
