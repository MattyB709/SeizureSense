{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using CUDA device 0\n",
      "Enabling CUDA with 6.94 GB available memory\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import json\n",
    "import time\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "mne.cuda.init_cuda(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to read our json file of each dictionary mapped to its file name\n",
    "def read_dict_from_json_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHBData(Dataset):\n",
    "    def __init__(self, patient_number, alpha=0.1, ):\n",
    "        self.patient_number = patient_number\n",
    "        self.segment_dict = read_dict_from_json_file(f'CHB-MIT/Segment_dicts/segment_dict{self.patient_number}.json')\n",
    "        # Mapping label names to integers\n",
    "        self.label_to_int = {'interictal': 0, 'preictal': 1, 'ictal': 2}\n",
    "        self.alpha = alpha  # Smoothing factor for EMA. Smaller alpha places greater importance on more recent elements\n",
    "        # Initialize EMA and variance (for standard deviation calculation) for each channel\n",
    "        self.EMA = {}\n",
    "        self.variance = {}\n",
    "        # Initial values could be adjusted based on dataset characteristics\n",
    "        self.initialized_channels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segment_dict)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segment_filepath = f'CHB-MIT/Segments/chb{self.patient_number}/{index}-chb{self.patient_number}.pt'\n",
    "        segment = torch.load(segment_filepath)\n",
    "        segment = segment.to(device)\n",
    "\n",
    "                # Ensure EMA and variance are initialized\n",
    "        if not self.initialized_channels:\n",
    "            self.initialize_ema_variance(segment.shape[1])\n",
    "            self.initialized_channels = True\n",
    "\n",
    "        # Computes z-normalization based on Exponential moving average\n",
    "        self.ema_normalize(segment)\n",
    "\n",
    "        segment = self.pad_sequence(segment)\n",
    "        \n",
    "\n",
    "        # Retrieve the label and map it to an integer\n",
    "        label_name = self.segment_dict[segment_filepath]\n",
    "        label = self.label_to_int[label_name]\n",
    "        \n",
    "        segment = segment.float()  # Assuming you're using a device in the outer scope\n",
    "\n",
    "        return segment, label\n",
    "\n",
    "    def pad_sequence(self, segment):\n",
    "        total_padding = 28 - segment.shape[1]\n",
    "        pad_top = total_padding // 2\n",
    "        pad_bottom = total_padding - pad_top\n",
    "        padded_tensor = F.pad(segment, (0, 0, pad_top, pad_bottom), mode='constant', value=0)\n",
    "        return padded_tensor\n",
    "\n",
    "    def initialize_ema_variance(self, num_channels):\n",
    "        for i in range(num_channels):\n",
    "            self.EMA[i] = None\n",
    "            self.variance[i] = None\n",
    "\n",
    "    def ema_normalize(self, segment):\n",
    "        # Update EMA and standard deviation for each channel and normalize\n",
    "        for i in range(segment.shape[1]):  \n",
    "            channel_data = segment[0, i, :] # select each channel\n",
    "            if self.EMA[i] is None:  # First update\n",
    "                self.EMA[i] = channel_data.mean()\n",
    "                self.variance[i] = channel_data.var()\n",
    "            else:\n",
    "                #update EMA and variance based\n",
    "                self.EMA[i] = self.alpha * channel_data.mean() + (1 - self.alpha) * self.EMA[i]\n",
    "                self.variance[i] = self.alpha * ((channel_data - self.EMA[i])**2).mean() + (1 - self.alpha) * self.variance[i]\n",
    "\n",
    "            std = torch.sqrt(self.variance[i])\n",
    "            # Normalize this channel\n",
    "            segment[0,i, :] = (channel_data - self.EMA[i]) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeizureSense(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SeizureSense,self).__init__()\n",
    "        \n",
    "        #Block 1\n",
    "        #should be taking in an input of 23x512\n",
    "        #first layer temporal filters\n",
    "        self.conv1=nn.Conv2d(1,8,(1,128),stride=1,padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8, False)\n",
    "        \n",
    "        #spatial layer(depthwise layer)\n",
    "        self.conv2_23=nn.Conv2d(8,32,(28,1))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, False)\n",
    "        self.avgpool1 = nn.AvgPool2d((1,2))\n",
    "        #apply dropout here in forward\n",
    "        \n",
    "        \n",
    "        #Block 2\n",
    "        #sepereable convolutional 2d\n",
    "        self.conv3=nn.Conv2d(32,32,(1,16),stride=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32,False) #CHECK THIS!\n",
    "        self.avgpool2 = nn.AvgPool2d((1,16))\n",
    "        \n",
    "     \n",
    "        #Block 3\n",
    "        \n",
    "        self.fc1= nn.Linear(96, 30)\n",
    "        self.fc2=nn.Linear(30,3)\n",
    "\n",
    "        #apply dropout here in forward\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)    \n",
    "        x = self.conv2_23(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool1(x)\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeizureSense().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "23",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start timing\u001b[39;00m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\mne\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\mne\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\mne\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\mne\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\mne\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mCHBData.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Computes z-normalization based on Exponential moving average\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_sequence(segment)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Retrieve the label and map it to an integer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mCHBData.ema_normalize\u001b[1;34m(self, segment)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(segment\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):  \n\u001b[0;32m     56\u001b[0m     channel_data \u001b[38;5;241m=\u001b[39m segment[\u001b[38;5;241m0\u001b[39m, i, :] \u001b[38;5;66;03m# select each channel\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEMA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# First update\u001b[39;00m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEMA[i] \u001b[38;5;241m=\u001b[39m channel_data\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance[i] \u001b[38;5;241m=\u001b[39m channel_data\u001b[38;5;241m.\u001b[39mvar()\n",
      "\u001b[1;31mKeyError\u001b[0m: 23"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([1,9,9], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "num_classes = 3\n",
    "num_patients = 16\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for patient_number in range(1,num_patients):\n",
    "        #ensures correct formatting for dataset\n",
    "        if patient_number < 10:\n",
    "            full_dataset = CHBData(f\"0{patient_number}\")\n",
    "        else:\n",
    "            full_dataset = CHBData(patient_number)\n",
    "\n",
    "        # Split the dataset into train and test sets\n",
    "        total_samples = len(full_dataset)\n",
    "        split_point = int(total_samples * 0.7)  # For a 70-30 split\n",
    "\n",
    "        train_indices = list(range(0, split_point))\n",
    "        val_indices = list(range(split_point, total_samples))\n",
    "\n",
    "        # dataloaders for training and validation\n",
    "        train_dataset = Subset(full_dataset, train_indices)\n",
    "        val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "        # Instantiate dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_probabilities = []  # Store probabilities for AUROC calculation\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                labels = labels.to(device)\n",
    "                logits = model(inputs)\n",
    "                outputs = F.softmax(logits, dim=1)\n",
    "\n",
    "                positive_class_probabilities = outputs[:, 1]\n",
    "                _, predictions = torch.max(outputs, 1) \n",
    "                predictions = predictions.cpu().numpy()\n",
    "                val_labels.extend(labels.cpu().numpy())  # Convert to numpy array\n",
    "                val_predictions.extend(predictions)\n",
    "                #val_probabilities.extend(outputs.cpu().numpy())  # Convert to numpy array\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    recall = recall_score(val_labels, val_predictions, average=None, zero_division=0)  \n",
    "    # Calculate AUROC\n",
    "    # For multi-class AUROC, adjust according to your class strategy\n",
    "    #val_labels_binarized = label_binarize(val_labels, classes=np.unique(val_labels))\n",
    "    #auroc = roc_auc_score(val_labels_binarized, np.array(val_probabilities), average='weighted',multi_class='ovr')  \n",
    "    # Timing end and calculate duration\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {np.mean(losses):.4f}, Accuracy: {accuracy:.4f}, Recall: {recall[1]:.4f}, Time: {epoch_duration:.2f}s')\n",
    "        #AUROC: {auroc:.4f},\n",
    "        # Reset losses for next epoch\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'SeizureSenseStateDict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_threshold=.3 #actual=.087\n",
    "def decode_predictions(predictions):\n",
    "    agregated_outputs=predictions.mean(dim=1)\n",
    "    probabilities=F.softmax(agregated_outputs,dim=-1)\n",
    "    print(probabilities)\n",
    "    if probabilities[0][1]>optimal_threshold:\n",
    "        return 'preictal'\n",
    "    else:\n",
    "        max_values, max_indices = torch.max(probabilities, dim=1)\n",
    "        if max_indices==0:\n",
    "            return 'interictal'\n",
    "        else:\n",
    "            return 'ictal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 3])\n",
      "tensor([[1., 0., 0.]], device='cuda:0')\n",
      "interictal\n"
     ]
    }
   ],
   "source": [
    "input=torch.rand((1,1,28,256)).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    predictions = model(input.to(device))\n",
    "    print(predictions.shape)\n",
    "    #print(predictions)\n",
    "    \n",
    "    print(decode_predictions(predictions))\n",
    "    #predictions=decode_predictions(predictions)\n",
    "    #print('Predictions:',predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
